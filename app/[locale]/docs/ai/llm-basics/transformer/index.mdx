---
title: "Transformer"
description: "Transformer 架构与注意力机制相关资料"
date: "2024-01-16"
tags:
  - transformer
  - attention
  - qkv
  - multi-head-attention
  - tokenizer
  - positional-encoding
  - feedforward
  - normalization
  - decoding
---

## 核心论文与代码

- [Attention is All You Need (原始论文, 2017)](https://arxiv.org/abs/1706.03762)
- [Github 代码复现（PyTorch）](https://github.com/jadore801120/attention-is-all-you-need-pytorch)

## 关键概念

- Self-Attention 的 QKV 计算
- Scaled Dot-Product 的作用
- Multi-Head Attention 的原理
- 分词与 Tokenizer
- 词嵌入（Word Embedding）
- 位置编码（Positional Encoding）
- 注意力机制（Attention Mechanism）
- 前馈网络（Feed Forward Network）
- 掩码（Masking）
- 层归一化（Normalization）
- 解码技术（Decoding Techniques）

## 深入学习

- Transformer 论文逐段精读 【论文精读】

## 注意力机制学习资源

- 【高清中英字幕】2025 年吴恩达详细讲解 Transformer 工作原理
  - [原课程链接（DeepLearning.AI Short Courses）](https://www.deeplearning.ai/short-courses/how-transformer-llms-work/)
- 彻底弄清注意力机制
