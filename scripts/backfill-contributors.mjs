#!/usr/bin/env node
/**
 * @description é€šè¿‡ GitHub commits API æ‹‰å– app/docs ç›®å½•ä¸‹æ¯ä¸ªæ–‡æ¡£çš„è´¡çŒ®è€…
 * å¹¶å°†æ±‡æ€»ç»“æœå†™å…¥ JSON æ–‡ä»¶ï¼Œä»¥ä¾›äººå·¥å®¡æ ¸
 * è‹¥æ£€æµ‹åˆ° DATABASE_URLï¼ˆæˆ–æ˜¾å¼ä¼ å…¥ --sync-db=trueï¼‰ï¼Œåˆ™ä¼šæŠŠç»Ÿè®¡ç»“æœåŒæ­¥åˆ°æ•°æ®åº“ã€‚
 *
 * ç¯å¢ƒå˜é‡/CLI è¦†ç›–ï¼š
 * - GITHUB_TOKENï¼šå¯é€‰ï¼Œè®¾ç½®åä¼šå¢åŠ é€Ÿç‡é™åˆ¶
 * - GITHUB_OWNER / GITHUB_REPOï¼šè¦†ç›–é»˜è®¤ä»“åº“ (InvolutionHell/involutionhell.github.io)
 * - DOCS_DIRï¼šç›¸å¯¹äºä»“åº“çš„æ–‡æ¡£æ ¹ç›®å½•ï¼ˆé»˜è®¤å€¼ï¼šapp/docsï¼‰
 * - OUTPUTï¼šç›¸å¯¹äºä»“åº“çš„è¾“å‡º JSON è·¯å¾„ï¼ˆé»˜è®¤å€¼ï¼štmp/doc-contributors.jsonï¼‰
 * - --output=path / --owner=name / --repo=name / --docs=dirï¼šä¸ç¯å¢ƒå˜é‡è¦†ç›–ç›¸åŒ
 * - --sync-db=true|falseï¼šæ˜¾å¼å¼€å¯/å…³é—­æ•°æ®åº“åŒæ­¥ï¼ˆé»˜è®¤ï¼šå­˜åœ¨ DATABASE_URL æ—¶å¼€å¯ï¼‰
 * - --skip-dbï¼šå¿«æ·æ–¹å¼ï¼Œç­‰ä»·äº --sync-db=false
 *
 * ç”¨æ³•ï¼š
 * pnpm exec node scripts/backfill-contributors.mjs
 * @author Siz Long
 * @date 2025-09-27
 * @location scripts/backfill-contributors.mjs
 */
import fs from "node:fs/promises";
import path from "node:path";
import process from "node:process";
import { fileURLToPath } from "node:url";
import "dotenv/config";
import { PrismaClient } from "../generated/prisma/index.js";

import fg from "fast-glob";
import matter from "gray-matter";

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);
const REPO_ROOT = path.resolve(__dirname, "..");

// è§£æå½¢å¦‚ --key=value çš„å‘½ä»¤è¡Œå‚æ•°ï¼Œå¹¶å½’ä¸€åŒ–ä¸ºå°å†™ key
const args = Object.fromEntries(
  process.argv
    .slice(2)
    .filter((arg) => arg.startsWith("--"))
    .map((arg) => {
      const [rawKey, rawValue] = arg.replace(/^--/, "").split("=");
      const key = rawKey.trim().toLowerCase();
      const value = rawValue === undefined ? "true" : rawValue.trim();
      return [key, value];
    }),
);

// åŸºç¡€é…ç½®ï¼šä»“åº“è·¯å¾„ã€æ–‡æ¡£ç›®å½•ã€è¾“å‡ºæ–‡ä»¶ä»¥åŠå•é¡µ commit æ•°é‡
const OWNER = args.owner || process.env.GITHUB_OWNER || "InvolutionHell";
const REPO = args.repo || process.env.GITHUB_REPO || "involutionhell.github.io";
const DOCS_DIR = args.docs || process.env.DOCS_DIR || "app/docs";
const OUTPUT = args.output || process.env.OUTPUT || "tmp/doc-contributors.json";
const PER_PAGE = Math.min(
  Math.max(Number(process.env.GITHUB_PER_PAGE) || 100, 1),
  100,
);
const TOKEN = process.env.GITHUB_TOKEN || "";

// æ•°æ®åº“åŒæ­¥å¼€å…³ï¼šé»˜è®¤åœ¨å­˜åœ¨ DATABASE_URL æ—¶å¯ç”¨ï¼Œå¯ç”¨ --skip-db ç¦æ­¢ï¼Œæˆ– --sync-db=true å¼ºåˆ¶å¯ç”¨
const argSyncDb = args["sync-db"];
const shouldSyncDb = (() => {
  if (args["skip-db"] === "true") return false;
  if (argSyncDb === "false") return false;
  if (argSyncDb === "true") return true;
  return Boolean(process.env.DATABASE_URL);
})();

let prisma = null;
if (shouldSyncDb) {
  prisma = new PrismaClient();
}

// é¢„å…ˆè®¡ç®—ç»å¯¹è·¯å¾„ä¸è¯·æ±‚å¤´ï¼Œæ–¹ä¾¿åç»­è°ƒç”¨
const docsDirAbs = path.resolve(REPO_ROOT, DOCS_DIR);
const outputAbs = path.resolve(REPO_ROOT, OUTPUT);
const headers = {
  "User-Agent": "involutionhell-contrib-backfill-script",
  Accept: "application/vnd.github+json",
  ...(TOKEN ? { Authorization: `Bearer ${TOKEN}` } : {}),
};

// ç»Ÿä¸€æ—¥å¿—è¾“å‡ºå‰ç¼€ï¼Œæ–¹ä¾¿å®šä½è„šæœ¬æ‰§è¡Œä¿¡æ¯
function log(...args) {
  console.log("[backfill-contributors]", ...args);
}

async function ensureParentDir(filePath) {
  // ä¿è¯è¾“å‡ºæ–‡ä»¶æ‰€åœ¨ç›®å½•å­˜åœ¨ï¼Œé¿å…å†™å…¥å¤±è´¥
  const dir = path.dirname(filePath);
  await fs.mkdir(dir, { recursive: true });
}

async function listDocFiles() {
  // ä½¿ç”¨ glob åŒ¹é… docs ç›®å½•ä¸‹çš„æ‰€æœ‰ Markdown/MDX æ–‡ä»¶
  const patterns = ["**/*.{md,mdx,markdown}"];
  const files = await fg(patterns, {
    cwd: docsDirAbs,
    onlyFiles: true,
    dot: false,
  });
  return files
    .map((relative) => ({
      relative,
      absolute: path.join(docsDirAbs, relative),
    }))
    .sort((a, b) => a.relative.localeCompare(b.relative));
}

function parseDocFrontmatter(content) {
  // è§£æ frontmatterï¼Œæå– docId ä¸æ ‡é¢˜ç­‰å…³é”®ä¿¡æ¯
  const parsed = matter(content);
  const data = parsed.data || {};
  const docId = typeof data.docId === "string" ? data.docId.trim() : "";
  const title = typeof data.title === "string" ? data.title.trim() : "";
  return {
    docId: docId || null,
    title: title || null,
    frontmatter: data,
  };
}

async function fetchCommitsForFile(repoRelativePath) {
  // å¾ªç¯åˆ†é¡µè¯·æ±‚ GitHub commits APIï¼Œç›´åˆ°æ²¡æœ‰ä¸‹ä¸€é¡µ
  const commits = [];
  let page = 1;

  while (true) {
    const url = new URL(
      `https://api.github.com/repos/${OWNER}/${REPO}/commits`,
    );
    url.searchParams.set("path", repoRelativePath);
    url.searchParams.set("per_page", String(PER_PAGE));
    url.searchParams.set("page", String(page));

    const res = await fetch(url, {
      headers,
    });

    if (res.status === 403) {
      // å‘½ä¸­é€Ÿç‡é™åˆ¶æ—¶æŠ›å‡ºè¯¦ç»†æŠ¥é”™ï¼Œä¾¿äºæç¤ºç­‰å¾…æ—¶é—´
      const reset = res.headers.get("x-ratelimit-reset");
      const resetDate = reset
        ? new Date(Number(reset) * 1000).toISOString()
        : "unknown";
      throw new Error(
        `GitHub API rate limit reached (path: ${repoRelativePath}). Resets at ${resetDate}.`,
      );
    }

    if (!res.ok) {
      const text = await res.text();
      throw new Error(
        `Failed to fetch commits for ${repoRelativePath}: ${res.status} ${res.statusText} -> ${text}`,
      );
    }

    const data = await res.json();
    if (!Array.isArray(data) || data.length === 0) {
      break;
    }

    commits.push(...data);

    const link = res.headers.get("link") || "";
    const hasNext = link.split(",").some((part) => part.includes('rel="next"'));
    if (!hasNext) {
      break;
    }
    page += 1;
  }

  return commits;
}

function aggregateContributors(commits) {
  // æ±‡æ€»è´¡çŒ®è€…ä¿¡æ¯å¹¶æŒ‰è´¡çŒ®æ¬¡æ•°æ’åºï¼ŒåŒæ—¶è®°å½•åŒ¿åæäº¤æ•°é‡
  const contributors = new Map();
  let skipped = 0;

  for (const commit of commits) {
    const author = commit?.author;
    if (!author || typeof author.id !== "number") {
      skipped += 1;
      continue;
    }

    const commitMeta = commit?.commit || {};
    const rawDate = commitMeta?.author?.date || commitMeta?.committer?.date;
    const commitDate = rawDate ? new Date(rawDate) : null;

    if (!contributors.has(author.id)) {
      contributors.set(author.id, {
        githubId: author.id,
        login: author.login || null,
        avatarUrl: author.avatar_url || null,
        htmlUrl: author.html_url || null,
        contributions: 1,
        lastContributedAt: commitDate ? commitDate.toISOString() : null,
      });
      continue;
    }

    const existing = contributors.get(author.id);
    existing.contributions += 1;
    if (commitDate) {
      // ä»…åœ¨å‘ç°æ›´æ™šçš„æäº¤æ—¶é—´æ—¶æ›´æ–° lastContributedAt
      const previous = existing.lastContributedAt
        ? new Date(existing.lastContributedAt)
        : null;
      if (!previous || commitDate > previous) {
        existing.lastContributedAt = commitDate.toISOString();
      }
    }
  }

  const sorted = Array.from(contributors.values()).sort((a, b) => {
    if (b.contributions !== a.contributions) {
      return b.contributions - a.contributions;
    }
    const dateA = a.lastContributedAt ? Date.parse(a.lastContributedAt) : 0;
    const dateB = b.lastContributedAt ? Date.parse(b.lastContributedAt) : 0;
    return dateB - dateA;
  });

  return { contributors: sorted, skipped };
}

function toSafeNumber(value) {
  if (typeof value !== "number" || !Number.isFinite(value)) return 0;
  return Math.round(value);
}

function mergeStatsInto(targetStats, extraStats) {
  for (const [k, v] of Object.entries(extraStats || {})) {
    targetStats[k] = (targetStats[k] ?? 0) + toSafeNumber(v);
  }
}

function maxDate(a, b) {
  if (!a) return b || null;
  if (!b) return a || null;
  return new Date(a) > new Date(b) ? a : b;
}

// 1) æ–°å¢ï¼šå»é‡åˆå¹¶å¤šè·¯å¾„çš„ commits
async function fetchCommitsForPaths(allPaths) {
  const bySha = new Map(); // sha -> commitObject
  for (const p of allPaths) {
    const commits = await fetchCommitsForFile(p);
    for (const c of commits) {
      const sha = c?.sha;
      if (sha && !bySha.has(sha)) bySha.set(sha, c);
    }
  }
  return Array.from(bySha.values());
}

// 2) æ–°å¢ï¼šæŠŠå½“å‰æ‰«æåˆ°çš„è·¯å¾„å†™å…¥ doc_paths
async function upsertDocPath(docId, repoRelativePath, title = null) {
  if (!prisma) return;

  await prisma.$transaction(async (tx) => {
    // 1) å…ˆç¡®ä¿çˆ¶è¡¨ docs å­˜åœ¨ï¼ˆæœ€å°åˆ›å»ºï¼‰
    await tx.docs.upsert({
      where: { id: docId },
      create: {
        id: docId,
        path_current: repoRelativePath ?? null,
        title: title ?? null,
      },
      update: {
        // æœ‰å°±ä¸å¼ºè¡Œè¦†ç›– titleï¼›åªåœ¨ä½ æƒ³è¦†ç›–æ—¶å†åŠ 
        path_current: repoRelativePath ?? undefined,
      },
    });

    // 2) å†å†™å­è¡¨ doc_pathsï¼ˆå¤åˆä¸»é”® upsertï¼‰
    await tx.doc_paths.upsert({
      where: { doc_id_path: { doc_id: docId, path: repoRelativePath } },
      create: { doc_id: docId, path: repoRelativePath },
      update: {},
    });
  });
}

// 3) æ–°å¢ï¼šæ‰¹é‡å–å‡ºä¸€ç»„ docId çš„å†å²è·¯å¾„
async function getAllPathsForDocIds(docIds) {
  if (!prisma || docIds.length === 0) return new Map();
  const rows = await prisma.doc_paths.findMany({
    where: { doc_id: { in: docIds } },
    select: { doc_id: true, path: true },
  });
  const map = new Map(); // docId -> Set(paths)
  for (const r of rows) {
    const s = map.get(r.doc_id) ?? new Set();
    s.add(r.path);
    map.set(r.doc_id, s);
  }
  return map;
}

async function syncResultsToDatabase(results) {
  if (!prisma) {
    log("Skipping database syncï¼šæœªæ£€æµ‹åˆ° DATABASE_URL æˆ–å·²æ˜¾å¼ç¦ç”¨ã€‚");
    return;
  }
  log("å¼€å§‹åŒæ­¥è´¡çŒ®è€…ä¿¡æ¯åˆ°æ•°æ®åº“ï¼ˆå¹‚ç­‰å¿«ç…§ï¼‰â€¦â€¦");

  // 1) å…ˆæŒ‰ docId èšåˆï¼ˆæŠŠå¤šä¸ª filePath çš„ç»Ÿè®¡åˆå¹¶ï¼‰
  // byDocId = Map<docId, { stats: Record<gid, count>, lastByUser: Record<gid, isoString>, lastFilePath?: string, title?: string }>
  const byDocId = new Map();

  for (const r of results) {
    if (r.error || !r.docId) continue;
    const entry = byDocId.get(r.docId) ?? {
      stats: {},
      lastByUser: {},
      lastFilePath: r.filePath,
      title: r.title ?? null,
    };
    // ç´¯åŠ ç»Ÿè®¡
    mergeStatsInto(entry.stats, r.contributorStats || {});
    // è®°å½•æ¯ä¸ªç”¨æˆ·çš„æœ€å¤§æ—¶é—´
    for (const c of r.contributors || []) {
      const k = String(c.githubId);
      entry.lastByUser[k] = maxDate(entry.lastByUser[k], c.lastContributedAt);
    }
    // ä»¥è¾ƒæ–°çš„è·¯å¾„ä½œä¸ºå½“å‰ pathï¼ˆå¯é€‰ï¼‰
    entry.lastFilePath = r.filePath || entry.lastFilePath;
    if (r.title) entry.title = r.title;
    byDocId.set(r.docId, entry);
  }

  // 2) è½åº“ï¼ˆå¹‚ç­‰è¦†ç›–ï¼‰
  let docsProcessed = 0;
  let contributorsUpserted = 0;

  for (const [docId, payload] of byDocId) {
    const { stats, lastByUser, lastFilePath, title } = payload;

    // upsert æ–‡æ¡£ï¼ˆä¸ä¸æ—§å€¼ç›¸åŠ ï¼Œç›´æ¥è¦†ç›– contributor_stats ä¸ºèšåˆå¿«ç…§ï¼‰
    await prisma.docs.upsert({
      where: { id: docId },
      create: {
        id: docId,
        path_current: lastFilePath ?? null,
        title: title ?? null,
        contributor_stats: stats, // â† å¿«ç…§è¦†ç›–ï¼ˆå¹‚ç­‰ï¼‰
      },
      update: {
        path_current: lastFilePath ?? undefined,
        title: title ?? undefined,
        contributor_stats: stats, // â† å¿«ç…§è¦†ç›–ï¼ˆå¹‚ç­‰ï¼‰
      },
    });

    // upsert æ˜ç»†è¡¨ï¼šæ¯ä¸ªç”¨æˆ·ä¸€æ¡ï¼Œæ•°é‡ = èšåˆæ€»æ•°ï¼Œæ—¶é—´ = å„è·¯å¾„çš„æœ€å¤§æ—¶é—´ï¼ˆè‹¥æ— åˆ™ç”¨ nowï¼‰
    for (const [gidStr, count] of Object.entries(stats)) {
      const last = lastByUser[gidStr]
        ? new Date(lastByUser[gidStr])
        : new Date();
      await prisma.doc_contributors.upsert({
        where: {
          doc_id_github_id: {
            doc_id: docId,
            github_id: BigInt(gidStr),
          },
        },
        create: {
          doc_id: docId,
          github_id: BigInt(gidStr),
          contributions: toSafeNumber(count),
          last_contributed_at: last,
        },
        update: {
          contributions: toSafeNumber(count),
          last_contributed_at: last,
        },
      });
      contributorsUpserted += 1;
      log(
        `  â†³ upsert ${docId} / user=${gidStr} â†’ count=${count}, last=${last.toISOString()}`,
      );
    }

    docsProcessed += 1;
  }

  log(
    `æ•°æ®åº“åŒæ­¥å®Œæˆï¼šå¤„ç† ${docsProcessed} ç¯‡æ–‡æ¡£ï¼Œè´¡çŒ®è€… upsert ${contributorsUpserted} æ¡ã€‚`,
  );
}

async function main() {
  // 1) æ‰«æä»“åº“ï¼Œæ”¶é›†å½“å‰ docId â†’ è·¯å¾„é›†åˆ & æ ‡é¢˜
  log(`Scanning docs from ${path.relative(REPO_ROOT, docsDirAbs)}`);
  const docFiles = await listDocFiles();
  if (docFiles.length === 0) {
    log("No doc files found. Abort.");
    return;
  }

  /** Map<string, Set<string>>: docId -> å½“å‰æ‰«æåˆ°çš„è·¯å¾„é›†åˆ */
  const currentDocIdPaths = new Map();
  /** Map<string, string|null>: docId -> æ ‡é¢˜ï¼ˆä»»å–ä¸€ä¸ªæ–‡ä»¶é‡Œçš„ titleï¼‰ */
  const titleByDocId = new Map();

  for (let i = 0; i < docFiles.length; i += 1) {
    const file = docFiles[i];
    const repoRelative = path
      .relative(REPO_ROOT, file.absolute)
      .replace(/\\/g, "/");

    const raw = await fs.readFile(file.absolute, "utf8");
    const meta = parseDocFrontmatter(raw);

    if (!meta.docId) {
      log(`  âš ï¸ è·³è¿‡ ${repoRelative}ï¼šç¼ºå°‘ docId`);
      continue;
    }

    // æ”¶é›†è·¯å¾„
    const set = currentDocIdPaths.get(meta.docId) ?? new Set();
    set.add(repoRelative);
    currentDocIdPaths.set(meta.docId, set);

    // æ”¶é›†ä¸€ä¸ªæ ‡é¢˜ï¼ˆå¯é€‰ï¼‰
    if (!titleByDocId.has(meta.docId) && meta.title) {
      titleByDocId.set(meta.docId, meta.title);
    }

    // æŠŠå½“å‰è·¯å¾„è®°å…¥ doc_pathsï¼ˆDB ä¸­ç»´æŠ¤å†å²è·¯å¾„ï¼‰
    await upsertDocPath(meta.docId, repoRelative);
  }

  // æ²¡æœ‰ä»»ä½•å« docId çš„æ–‡ä»¶ï¼Œç›´æ¥ç»“æŸ
  const docIds = Array.from(currentDocIdPaths.keys());
  if (docIds.length === 0) {
    log("No docs with docId were found. Abort.");
    return;
  }

  // 2) ä»æ•°æ®åº“å–æ¯ä¸ª docId çš„å†å²è·¯å¾„ï¼Œå¹¶ä¸å½“å‰è·¯å¾„åšå¹¶é›†
  const historical = await getAllPathsForDocIds(docIds);

  // 3) å¯¹æ¯ä¸ª docIdï¼šå¯¹â€œæ‰€æœ‰ï¼ˆå†å²+å½“å‰ï¼‰è·¯å¾„â€æŠ“ commitsï¼ˆæŒ‰ sha å»é‡ï¼‰ï¼Œå†èšåˆä½œè€…ç»Ÿè®¡
  const results = [];

  for (const docId of docIds) {
    const currentSet = currentDocIdPaths.get(docId) ?? new Set();
    const histSet = historical.get(docId) ?? new Set();
    const unionPaths = new Set([...histSet, ...currentSet]);

    // æ²¡è·¯å¾„å°±è·³è¿‡
    if (unionPaths.size === 0) {
      log(`  âš ï¸ docId=${docId} æ— è·¯å¾„è®°å½•ï¼Œè·³è¿‡`);
      continue;
    }

    // é€‰ä¸€ä¸ªä»£è¡¨æ€§çš„è·¯å¾„æ”¾åˆ° result.filePathï¼ˆç”¨äºå®¡è®¡/å›æ˜¾ï¼‰
    const representativePath =
      currentSet.values().next().value ?? histSet.values().next().value ?? null;
    const title = titleByDocId.get(docId) ?? null;

    let commits = [];
    try {
      // ğŸ‘‡ å…³é”®ï¼šå¯¹â€œæ‰€æœ‰è·¯å¾„â€æŠ“å–å¹¶æŒ‰ SHA å»é‡ï¼Œé¿å…é‡å‘½åé€ æˆåŒè®¡
      commits = await fetchCommitsForPaths(Array.from(unionPaths));
    } catch (err) {
      log(`  âœ– æ‹‰å– commits å¤±è´¥ (docId=${docId}): ${err.message}`);
      results.push({
        docId,
        title,
        filePath: representativePath,
        allPaths: Array.from(unionPaths),
        error: err.message,
      });
      continue;
    }

    // å¤ç”¨ä½ åŸæœ‰çš„ä½œè€…èšåˆé€»è¾‘
    const { contributors, skipped } = aggregateContributors(commits);

    const stats = Object.fromEntries(
      (contributors || []).map((c) => [String(c.githubId), c.contributions]),
    );

    const commitTimestamps = commits
      .map((c) => c?.commit?.author?.date || c?.commit?.committer?.date)
      .filter(Boolean)
      .map((v) => Date.parse(v))
      .filter(Number.isFinite)
      .sort((a, b) => b - a);

    results.push({
      docId,
      title,
      filePath: representativePath,
      allPaths: Array.from(unionPaths), // ä¾¿äºå®¡è®¡/æ’æŸ¥
      totalCommits: commits.length,
      skippedCommits: skipped,
      contributors,
      contributorStats: stats, // {"githubId": count}
      lastCommitAt: commitTimestamps.length
        ? new Date(commitTimestamps[0]).toISOString()
        : null,
    });
  }

  // 4) å†™ JSON å¿«ç…§ï¼ˆæŠ“å–å±‚çš„ç»“æœï¼›ä¸ DB åŒæ­¥åçš„çŠ¶æ€åº”ä¸€è‡´ï¼‰
  await ensureParentDir(outputAbs);
  await fs.writeFile(
    outputAbs,
    JSON.stringify(
      {
        repo: `${OWNER}/${REPO}`,
        generatedAt: new Date().toISOString(),
        docsDir: path.relative(REPO_ROOT, docsDirAbs),
        totalDocs: results.length,
        results,
      },
      null,
      2,
    ),
  );
  log(
    `Done. Wrote ${results.length} doc entries to ${path.relative(REPO_ROOT, outputAbs)}`,
  );

  // 5) å¹‚ç­‰è½åº“ï¼šdocs.contributor_stats è¦†ç›–å¿«ç…§ï¼›doc_contributors æ˜ç»† upsertï¼ˆè®¡æ•°=åˆå¹¶åçš„æ€»æ•°ï¼Œæ—¶é—´=æœ€å¤§ï¼‰
  await syncResultsToDatabase(results);
}

// Step 3: æ•è·æœªå¤„ç†å¼‚å¸¸ï¼Œé¿å…è„šæœ¬é™é»˜å´©æºƒ
main()
  .catch((error) => {
    console.error("[backfill-contributors] Unexpected error", error);
    process.exit(1);
  })
  .finally(async () => {
    if (prisma) {
      await prisma.$disconnect();
    }
  });
